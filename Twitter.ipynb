{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SQLite"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from ipywidgets import widgets\r\n",
    "from IPython.display import display\r\n",
    "import time\r\n",
    "import json\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import glob\r\n",
    "from sqlalchemy import create_engine\r\n",
    "import string\r\n",
    "import re\r\n",
    "import contractions\r\n",
    "from emoji import UNICODE_EMOJI\r\n",
    "string.punctuation\r\n",
    "import nltk\r\n",
    "\r\n",
    "def remove_Atsign(text):\r\n",
    "    \"\"\"Remove at sign from the input Text\"\"\"\r\n",
    "    return re.sub(r\"@\\S+\", \"\", text)\r\n",
    "\r\n",
    "def remove_URL(text):\r\n",
    "    \"\"\"Remove URL from the input Text\"\"\"\r\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\r\n",
    "\r\n",
    "def remove_Contraction(text):\r\n",
    "    \"\"\"Remove Contraction from the input Text\"\"\"\r\n",
    "    try:\r\n",
    "        text = contractions.fix(text)\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "    return(text)\r\n",
    "\r\n",
    "def remove_Emoji(text):\r\n",
    "    \"\"\"Remove Emoji\"\"\"\r\n",
    "    return ''.join(c for c in text if c not in UNICODE_EMOJI['en'])\r\n",
    "\r\n",
    "def remove_GUID(text):\r\n",
    "    \"\"\"Remove URL from the input Text\"\"\"\r\n",
    "    return re.sub(r\"[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\", \"\", text)\r\n",
    "\r\n",
    "def remove_Numbers(text):\r\n",
    "    \"\"\"Remove Numbers from the input Text\"\"\"\r\n",
    "    remove_digits = str.maketrans('', '', string.digits)\r\n",
    "    return(text.translate(remove_digits))\r\n",
    "\r\n",
    "def remove_Punctuation(text):\r\n",
    "    \"\"\"Remove Punctuation from the input Text\"\"\"\r\n",
    "    \"\"\"!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\"\"\r\n",
    "    text_punctuated = \"\".join([char for char in text if char not in string.punctuation])\r\n",
    "    return text_punctuated\r\n",
    "\r\n",
    "def user_id(user):\r\n",
    "    try:\r\n",
    "        return(user['id'])\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "\r\n",
    "def user_location(user):\r\n",
    "    try:\r\n",
    "        return(user['location'])\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "\r\n",
    "def extended_tweet(extended):\r\n",
    "    try:\r\n",
    "        return(extended['full_text'])\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "\r\n",
    "def user_description(user):\r\n",
    "    try:\r\n",
    "        return(user['description'])\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "\r\n",
    "def tokenize_data(text):\r\n",
    "    \"\"\"Tokenizing input text\"\"\"\r\n",
    "    tokens = re.split('\\W+', text.lower())\r\n",
    "    return tokens\r\n",
    "\r\n",
    "def remove_Stopwords(text):\r\n",
    "    \"\"\"Removing Stops words\"\"\"\r\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\r\n",
    "    #Adding additional words to the list of stop words to remove them\r\n",
    "    #addition_StopWords = []\r\n",
    "    #stopwords.extend(addition_StopWords)\r\n",
    "    text_stopwords = [word for word in text if word not in stopwords]\r\n",
    "    return(text_stopwords)\r\n",
    "\r\n",
    "def lemmatizing(text):\r\n",
    "    \"\"\"Lemmatizing the input text using WordNet and NLTK package\"\"\"\r\n",
    "    NLTK_WNL = nltk.WordNetLemmatizer()\r\n",
    "    text_Lem = [NLTK_WNL.lemmatize(word) for word in text]\r\n",
    "    return(text_Lem)\r\n",
    "\r\n",
    "def tweet_cleaning(text):\r\n",
    "    \"\"\"Cleaning and Lemmatizing Tweets\"\"\"\r\n",
    "    text = remove_URL(text)\r\n",
    "    text = remove_Atsign(text)\r\n",
    "    text = remove_Emoji(text)\r\n",
    "    text = remove_GUID(text)\r\n",
    "    text = remove_Numbers(text)\r\n",
    "    text = remove_Contraction(text)\r\n",
    "    text = remove_Punctuation(text)\r\n",
    "    text = tokenize_data(text)\r\n",
    "    text = remove_Stopwords(text)\r\n",
    "    text = lemmatizing(text)\r\n",
    "    return(text)\r\n",
    "\r\n",
    "    \r\n",
    "datastore_Path = \"D:/F-drive-31578/Twitter-2020-12/All\"\r\n",
    "tweet_Cols = ['u_id', 'u_location', 'u_description', 'id', 'extended_tweet', 'retweeted', 'geo', 'coordinates', 'place']\r\n",
    "#tweet_Cols = ['u_id', 'u_location', 'u_description', 'id', 'text', 'retweeted', 'geo', 'coordinates', 'place']\r\n",
    "\r\n",
    "df_List = [] #An empty list to keep DataFrames\r\n",
    "json_Filelist = glob.glob(os.path.join(datastore_Path, '*.json'))\r\n",
    "\r\n",
    "\r\n",
    "prog = widgets.IntProgress(continuous_update=False, min=0, max=len(json_Filelist), \r\n",
    "                            description=  '0/' + str(len(json_Filelist)) + '  ', orientation='horizontal',\r\n",
    "                            style={'bar_style': 'success'})# instantiate the bar\r\n",
    "display(prog)# display the bar\r\n",
    "\r\n",
    "#db = sqlite3.connect(\"F:/Tweets/Tweets.sqlite\")\r\n",
    "engine = create_engine('sqlite:///C:\\Tweets\\\\Tweets_clean.db', echo = False)\r\n",
    "sqlite_connection = engine.connect()\r\n",
    "\r\n",
    "for file in json_Filelist:\r\n",
    "    df_Temp = pd.read_json(file, lines=True)\r\n",
    "    \r\n",
    "    df_Temp = df_Temp[df_Temp['lang'] == 'en']\r\n",
    "    df_Temp['u_id'] = df_Temp['user'].apply(lambda x: user_id(x))\r\n",
    "    \r\n",
    "    df_Temp['u_location'] = df_Temp['user'].apply(lambda x: user_location(x))\r\n",
    "    df_Temp = df_Temp[df_Temp['u_location'].notnull()]\r\n",
    "\r\n",
    "    df_Temp = df_Temp[df_Temp['extended_tweet'].notnull()]\r\n",
    "    df_Temp['extended_tweet'] = df_Temp['extended_tweet'].apply(lambda x: extended_tweet(x))\r\n",
    "\r\n",
    "    df_Temp['u_description'] = df_Temp['user'].apply(lambda x: user_description(x))\r\n",
    "    df_Temp['u_description'] = df_Temp['u_description'].apply(lambda x: tweet_cleaning(x))\r\n",
    "    \r\n",
    "    df_Temp['extended_tweet'] = df_Temp['extended_tweet'].apply(lambda x: tweet_cleaning(x))\r\n",
    "    \r\n",
    "    #df_Temp = df_Temp[df_Temp['extended_tweet'].str.len() >= 50]\r\n",
    "    df_Temp[tweet_Cols].astype(str).to_sql(\"Tweets\", sqlite_connection, if_exists=\"append\")\r\n",
    "    prog.value += 1 # signal to increment the progress bar\r\n",
    "    prog.description =  str(prog.value) + '/' + str(len(json_Filelist)) + '  '\r\n",
    "\r\n",
    "sqlite_connection.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "sqlite_connection.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "sqlite_connection = engine.connect()\r\n",
    "df11 = pd.read_sql('SELECT * FROM Tweets', sqlite_connection)\r\n",
    "df11.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index                u_id                     u_location  \\\n",
       "0     22          3232688427                 United Kingdom   \n",
       "1     40          3099677072            Islamabad, Pakistan   \n",
       "2     51          4755630367  2 Leman Street, London E1 8FA   \n",
       "3     53            14371794                Crewe, Cheshire   \n",
       "4     54  709368941592846336        Cambridgeshire, England   \n",
       "\n",
       "                                       u_description                      id  \\\n",
       "0  20 | Queer | He/They | I love RGG, DMC, MGS, S...   1.346380920534786e+18   \n",
       "1                                    ‚Äè‚Äè‚Äè‚Äè‚Äè‚Äèÿ¢ÿ™ŸÜ⁄ØŸàÿßÿØ€å!   1.346380920526426e+18   \n",
       "2  Insurance Business UK is a website with multip...  1.3463809205347942e+18   \n",
       "3  Singer/songwriter, promoter, environmentalist....  1.3463809205389926e+18   \n",
       "4  Healthy You in Cambridgeshire & Peterborough i...  1.3463809205348024e+18   \n",
       "\n",
       "                                                text  \\\n",
       "0  @JudgementKinsey Garfields third bomb: Bites t...   \n",
       "1  RT @loishh: the art                           ...   \n",
       "2  #Insurance companies have turned to #artificia...   \n",
       "3  On the twelfth day of Christmas is a cover of ...   \n",
       "4  As a lot of us are making #NewYearResolutions ...   \n",
       "\n",
       "                                      extended_tweet retweeted  geo  \\\n",
       "0                                                nan       0.0  nan   \n",
       "1                                                nan       0.0  nan   \n",
       "2  {'full_text': \"#Insurance companies have turne...       0.0  nan   \n",
       "3  {'full_text': \"On the twelfth day of Christmas...       0.0  nan   \n",
       "4  {'full_text': 'As a lot of us are making #NewY...       0.0  nan   \n",
       "\n",
       "  coordinates place  \n",
       "0         nan   nan  \n",
       "1         nan   nan  \n",
       "2         nan   nan  \n",
       "3         nan   nan  \n",
       "4         nan   nan  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>u_id</th>\n",
       "      <th>u_location</th>\n",
       "      <th>u_description</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>geo</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>3232688427</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>20 | Queer | He/They | I love RGG, DMC, MGS, S...</td>\n",
       "      <td>1.346380920534786e+18</td>\n",
       "      <td>@JudgementKinsey Garfields third bomb: Bites t...</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>3099677072</td>\n",
       "      <td>Islamabad, Pakistan</td>\n",
       "      <td>‚Äè‚Äè‚Äè‚Äè‚Äè‚Äèÿ¢ÿ™ŸÜ⁄ØŸàÿßÿØ€å!</td>\n",
       "      <td>1.346380920526426e+18</td>\n",
       "      <td>RT @loishh: the art                           ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>4755630367</td>\n",
       "      <td>2 Leman Street, London E1 8FA</td>\n",
       "      <td>Insurance Business UK is a website with multip...</td>\n",
       "      <td>1.3463809205347942e+18</td>\n",
       "      <td>#Insurance companies have turned to #artificia...</td>\n",
       "      <td>{'full_text': \"#Insurance companies have turne...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>14371794</td>\n",
       "      <td>Crewe, Cheshire</td>\n",
       "      <td>Singer/songwriter, promoter, environmentalist....</td>\n",
       "      <td>1.3463809205389926e+18</td>\n",
       "      <td>On the twelfth day of Christmas is a cover of ...</td>\n",
       "      <td>{'full_text': \"On the twelfth day of Christmas...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>709368941592846336</td>\n",
       "      <td>Cambridgeshire, England</td>\n",
       "      <td>Healthy You in Cambridgeshire &amp; Peterborough i...</td>\n",
       "      <td>1.3463809205348024e+18</td>\n",
       "      <td>As a lot of us are making #NewYearResolutions ...</td>\n",
       "      <td>{'full_text': 'As a lot of us are making #NewY...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PKL Combiner"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "list_dfs = []\r\n",
    "pkl_Path = \"F:/Tweets\"\r\n",
    "pkl_Filelist = glob.glob(os.path.join(pkl_Path, '*.pkl'))\r\n",
    "prog = widgets.IntProgress(continuous_update=False, min=0, max=len(pkl_Filelist), \r\n",
    "                            description=  '0 / ' + str(len(pkl_Filelist)) + '  ', orientation='horizontal',\r\n",
    "                            style={'bar_style': 'success'})# instantiate the bar\r\n",
    "display(prog)# display the bar\r\n",
    "#counter = 0\r\n",
    "for file in pkl_Filelist:\r\n",
    "    try:\r\n",
    "        list_dfs.append(pd.read_pickle(file))\r\n",
    "        #counter += 1\r\n",
    "        prog.value += 1 # signal to increment the progress bar\r\n",
    "        prog.description =  str(prog.value) + ' / ' + str(len(pkl_Filelist)) + '  '\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "\r\n",
    "\r\n",
    "combined_df = pd.concat(list_dfs)\r\n",
    "combined_df.to_pickle('F:/Tweets/combined.pkl')\r\n",
    "combined_df.head()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "IntProgress(value=0, description='0 / 3  ', max=3)"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50371716770243cfafd5f1759d06ac30"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              id                                               text  \\\n",
       "9   1.333697e+18  Y am I getting photoshoot and brand ambassador...   \n",
       "42  1.333697e+18  Have a good day, everyone (except you, Laurenc...   \n",
       "50  1.333697e+18  @lexsion @catturd2 @POTUS You need a better ho...   \n",
       "51  1.333697e+18  Each Tuesday in December we want to celebrate ...   \n",
       "52  1.333697e+18  üì¢ CALL FOR PARTICIPANTS | NYC Youth Mental Wel...   \n",
       "\n",
       "                                       extended_tweet                 u_id  \\\n",
       "9                                                 NaN  1017370991301832704   \n",
       "42                                                NaN             77269789   \n",
       "50                                                NaN   786704913447395328   \n",
       "51  {'full_text': 'Each Tuesday in December we wan...           4869678789   \n",
       "52  {'full_text': 'üì¢ CALL FOR PARTICIPANTS | NYC Y...            178367902   \n",
       "\n",
       "                     u_location  \\\n",
       "9                            ‚ô°‚Ä¢   \n",
       "42          Co. Durham, England   \n",
       "50                United States   \n",
       "51         Dublin City, Ireland   \n",
       "52  Republic of the Philippines   \n",
       "\n",
       "                                        u_description  retweeted  \n",
       "9                                      ‚Äîyolo s√∂ chile        0.0  \n",
       "42  Writer with bylines @ Digital Spy, SYFY, Scree...        0.0  \n",
       "50  Ex-Democrat 18 years. Part Republican, part Li...        0.0  \n",
       "51  SHARECITY is a @ERC_Research funded research p...        0.0  \n",
       "52                The Voice and Advocate of the Youth        0.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>u_id</th>\n",
       "      <th>u_location</th>\n",
       "      <th>u_description</th>\n",
       "      <th>retweeted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.333697e+18</td>\n",
       "      <td>Y am I getting photoshoot and brand ambassador...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1017370991301832704</td>\n",
       "      <td>‚ô°‚Ä¢</td>\n",
       "      <td>‚Äîyolo s√∂ chile</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.333697e+18</td>\n",
       "      <td>Have a good day, everyone (except you, Laurenc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77269789</td>\n",
       "      <td>Co. Durham, England</td>\n",
       "      <td>Writer with bylines @ Digital Spy, SYFY, Scree...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.333697e+18</td>\n",
       "      <td>@lexsion @catturd2 @POTUS You need a better ho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>786704913447395328</td>\n",
       "      <td>United States</td>\n",
       "      <td>Ex-Democrat 18 years. Part Republican, part Li...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.333697e+18</td>\n",
       "      <td>Each Tuesday in December we want to celebrate ...</td>\n",
       "      <td>{'full_text': 'Each Tuesday in December we wan...</td>\n",
       "      <td>4869678789</td>\n",
       "      <td>Dublin City, Ireland</td>\n",
       "      <td>SHARECITY is a @ERC_Research funded research p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.333697e+18</td>\n",
       "      <td>üì¢ CALL FOR PARTICIPANTS | NYC Youth Mental Wel...</td>\n",
       "      <td>{'full_text': 'üì¢ CALL FOR PARTICIPANTS | NYC Y...</td>\n",
       "      <td>178367902</td>\n",
       "      <td>Republic of the Philippines</td>\n",
       "      <td>The Voice and Advocate of the Youth</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multiple finder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "# duplicates finder\r\n",
    "def getDuplicateUsers(df):\r\n",
    "\tduplicateUsers = set()\r\n",
    "\tfor x in range(df.shape[1]):\r\n",
    "\t\tusr = df.iloc[x]['u_id']\r\n",
    "\t\tfor y in range(x + 1, df.shape[1]):\r\n",
    "\t\t\totherUsr = df.iloc[y]['u_id']\r\n",
    "\t\t\tif usr == otherUsr:\r\n",
    "\t\t\t\tduplicateUsers.add(usr)\r\n",
    "\treturn list(duplicateUsers)\r\n",
    "\r\n",
    "df_Tweets = pd.read_pickle('F:/Tweets/combined.pkl')\r\n",
    "duplicateColNames = getDuplicateUsers(df_Tweets)\r\n",
    "print('Duplicate Columns are :', duplicateColNames)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Duplicate Columns are : []\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "db = sqlite3.connect(\"F:/Tweets/Tweets.sqlite\")\r\n",
    "db.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "6bd3622f6f3f8bb6e8b4643a72dc4bf7a75a1467dea0f2ef918aabb1c737dfb1"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}